import tempfile

import soundfile as sf
import torch
from allophant import predictions
from allophant.dataset_processing import Batch
from allophant.estimator import Estimator
from allosaurus.app import read_recognizer
from espnet_basic_dataset import ESPnetBasicDataset
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor


class AllophantInference:
    def __init__(self, device="cpu", **kwargs):
        self.device = device
        self.model, self.attribute_indexer = Estimator.restore(
            "kgnlp/allophant", device=self.device
        )
        self.inventory = self.attribute_indexer.phoneme_inventory(
            languages=kwargs.get("language")
        )
        self.feature_matrix = self.attribute_indexer.composition_feature_matrix(
            self.inventory
        ).to(self.device)
        self.inventory_indexer = self.attribute_indexer.attributes.subset(
            self.inventory
        )
        self.feature_name = "phoneme"
        self.decoder = predictions.feature_decoders(
            self.inventory_indexer, feature_names=[self.feature_name]
        )[self.feature_name]

    def infer(self, input_batch):
        with torch.no_grad():
            audio_input = input_batch["wav"]
            assert (
                audio_input.shape[0] == 1
            ), "Batch size > 1 not supported for inference!"
            batch = Batch(
                audio_input, torch.tensor([audio_input.shape[1]]), torch.zeros(1)
            ).to(self.device)
            # .outputs['phoneme'] contains the logits over inventory
            model_outputs = self.model.predict(batch, self.feature_matrix)
            decoded = self.decoder(
                model_outputs.outputs[self.feature_name].transpose(1, 0),
                model_outputs.lengths,
            )
            for [hypothesis] in decoded:
                recognized = self.inventory_indexer.feature_values(
                    self.feature_name, hypothesis.tokens - 1
                )
                recognized = "".join(recognized)
        return recognized


class AllosaurusInference:
    def __init__(self, device="cpu", **kwargs):
        self.device = device
        self.model = read_recognizer()
        if device == "cuda" and torch.cuda.is_available():
            self.model.config.device_id = 0
            self.model.am.to("cuda")
        else:
            self.model.config.device_id = -1  # CPU

    def infer(self, input_batch):
        with torch.no_grad():
            audiopath = input_batch["wavpath"]
            if not audiopath.endswith(".wav"):
                with tempfile.NamedTemporaryFile(
                    suffix=".wav", delete=True
                ) as temp_wav:
                    sf.write(
                        temp_wav.name, input_batch["wav"].squeeze(0).numpy(), 16000
                    )
                    transcription = self.model.recognize(temp_wav.name)
            else:
                transcription = self.model.recognize(audiopath)
        recognized = "".join(transcription).replace("อก", "").replace(" ", "")
        return recognized


class Wav2Vec2PhonemeInference:
    def __init__(self, device="cpu", **kwargs):
        self.device = device
        self.model_path = kwargs.get(
            "model_path", "facebook/wav2vec2-lv-60-espeak-cv-ft"
        )
        self.processor = Wav2Vec2Processor.from_pretrained(self.model_path)
        # Load model with appropriate dtype based on device
        if device == "cuda" and torch.cuda.is_available():
            self.model = Wav2Vec2ForCTC.from_pretrained(
                self.model_path, torch_dtype=torch.float16
            )
            self.dtype = torch.float16
        else:
            self.model = Wav2Vec2ForCTC.from_pretrained(
                self.model_path, torch_dtype=torch.float32
            )
            self.dtype = torch.float32

        self.model.eval().to(device)

    def infer(self, input_batch):
        audio = input_batch["wav"].squeeze(0).numpy()
        inputs = self.processor(
            audio,
            sampling_rate=self.processor.feature_extractor.sampling_rate,
            return_tensors="pt",
            padding=True,
        )
        with torch.no_grad():
            input_values = inputs.input_values.to(self.device)
            input_values = input_values.to(self.dtype)
            if hasattr(inputs, "attention_mask"):
                attention_mask = inputs.attention_mask.to(self.device)
                logits = self.model(input_values, attention_mask=attention_mask).logits
            else:
                logits = self.model(input_values).logits
        predicted_ids = torch.argmax(logits, dim=-1)
        transcription = self.processor.batch_decode(predicted_ids)[0]
        recognized = "".join(transcription)
        recognized = recognized.replace("อก", "").replace(" ", "")
        return recognized


def get_inference_model(model_name, device="cpu", **kwargs):
    if model_name == "allophant":
        return AllophantInference(device=device, **kwargs)
    elif model_name == "allosaurus":
        return AllosaurusInference(device=device, **kwargs)
    elif model_name in [
        "facebook/wav2vec2-lv-60-espeak-cv-ft",
        "facebook/wav2vec2-xlsr-53-espeak-cv-ft",
        "ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns",
    ]:
        return Wav2Vec2PhonemeInference(model_path=model_name, device=device, **kwargs)
    else:
        raise ValueError(f"Unknown model name: {model_name}")
